\BOOKMARK [0][-]{tocmark.0}{Contents}{}% 1
\BOOKMARK [0][-]{chapter.1}{Atari Gym:Ice Hockey}{}% 2
\BOOKMARK [1][-]{section.1.1}{Gym}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.2}{Ice Hockey}{chapter.1}% 4
\BOOKMARK [1][-]{section.1.3}{Record:Ice Hockey}{chapter.1}% 5
\BOOKMARK [0][-]{chapter.2}{Learning from Demonstration}{}% 6
\BOOKMARK [1][-]{section.2.1}{Playing hard exploration games by watching YouTube}{chapter.2}% 7
\BOOKMARK [1][-]{section.2.2}{Player Experience Extraction from Gameplay Video}{chapter.2}% 8
\BOOKMARK [1][-]{section.2.3}{Learning Montezuma's Revenge from a Single Demonstration}{chapter.2}% 9
\BOOKMARK [1][-]{section.2.4}{Deep Q-learning from Demonstration}{chapter.2}% 10
\BOOKMARK [0][-]{chapter.3}{Object Detection}{}% 11
\BOOKMARK [1][-]{section.3.1}{My Application}{chapter.3}% 12
\BOOKMARK [1][-]{section.3.2}{Object Detection Pre-Train techniques}{chapter.3}% 13
\BOOKMARK [1][-]{section.3.3}{Object Detection Trainning techniques}{chapter.3}% 14
\BOOKMARK [1][-]{section.3.4}{Object-oriented state Abstraction in RL for Video Games}{chapter.3}% 15
\BOOKMARK [1][-]{section.3.5}{YOLO}{chapter.3}% 16
\BOOKMARK [1][-]{section.3.6}{CenterNet: Keypoint Triplets for Object Detection \046 Objects as Points}{chapter.3}% 17
\BOOKMARK [1][-]{section.3.7}{Object Tracking Vs. Object Detection}{chapter.3}% 18
\BOOKMARK [0][-]{chapter.4}{State Embeddings}{}% 19
\BOOKMARK [1][-]{section.4.1}{Dynamic image encoder}{chapter.4}% 20
\BOOKMARK [1][-]{section.4.2}{Stack of difference of frame video-clip encoder}{chapter.4}% 21
\BOOKMARK [1][-]{section.4.3}{Grad-CAM ++}{chapter.4}% 22
\BOOKMARK [1][-]{section.4.4}{VCG}{chapter.4}% 23
\BOOKMARK [0][-]{chapter.5}{RL}{}% 24
\BOOKMARK [1][-]{section.5.1}{Definitions}{chapter.5}% 25
\BOOKMARK [1][-]{section.5.2}{Off/Policy}{chapter.5}% 26
\BOOKMARK [1][-]{section.5.3}{On/Policy}{chapter.5}% 27
\BOOKMARK [1][-]{section.5.4}{Model Based}{chapter.5}% 28
\BOOKMARK [1][-]{section.5.5}{Model Free}{chapter.5}% 29
\BOOKMARK [1][-]{section.5.6}{DQN}{chapter.5}% 30
\BOOKMARK [1][-]{section.5.7}{A3C}{chapter.5}% 31
\BOOKMARK [0][-]{chapter.6}{IRL}{}% 32
\BOOKMARK [1][-]{section.6.1}{GAN}{chapter.6}% 33
\BOOKMARK [1][-]{section.6.2}{Autoencoder}{chapter.6}% 34
\BOOKMARK [1][-]{section.6.3}{Hidden Markov Model}{chapter.6}% 35
\BOOKMARK [1][-]{section.6.4}{Apprenticeship Learing}{chapter.6}% 36
\BOOKMARK [1][-]{section.6.5}{Bayesian Inverse Reinforcement Learning}{chapter.6}% 37
\BOOKMARK [1][-]{section.6.6}{Maximum Entropy Reinforcement Learning}{chapter.6}% 38
\BOOKMARK [1][-]{section.6.7}{Generative Adversarial Imitation Learning}{chapter.6}% 39
\BOOKMARK [1][-]{section.6.8}{Project Scope}{chapter.6}% 40
\BOOKMARK [1][-]{section.6.9}{Report Outline}{chapter.6}% 41
\BOOKMARK [0][-]{chapter.7}{Policy Optimisation}{}% 42
\BOOKMARK [1][-]{section.7.1}{Dynamic image encoder}{chapter.7}% 43
\BOOKMARK [1][-]{section.7.2}{Stack of difference of frame video-clip encoder}{chapter.7}% 44
\BOOKMARK [0][-]{section*.5}{Bibliography}{}% 45
